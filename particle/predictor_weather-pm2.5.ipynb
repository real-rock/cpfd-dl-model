{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0f37c-81af-4b3f-9b4b-5ab45fc071e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(\"../scripts/particles/\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775d93e-6262-4742-9cc4-dfd234b0670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handler as dh\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89c1c6-7349-4c85-892a-cd22bc346f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['PM1_2.5']\n",
    "inputs = [\n",
    "    'PM1_2.5_OUT',\n",
    "    'PM1_2.5_H_OUT',\n",
    "    'PERSON_NUMBER',\n",
    "    'AIR_PURIFIER',\n",
    "    'WINDOW',\n",
    "    'AIR_CONDITIONER',\n",
    "    'DOOR',\n",
    "    # 'TEMPERATURE',\n",
    "    'WIND_SPEED',\n",
    "    'WIND_DEG',\n",
    "    'HUMIDITY'\n",
    "]\n",
    "\n",
    "in_time_step = 60\n",
    "offset = 1\n",
    "out_time_step = 1\n",
    "batch_size = 32\n",
    "\n",
    "config = {\n",
    "    \"name\": \"pm2_5_conv\",\n",
    "    \"description\": \"conv1D pm2.5\",\n",
    "    \"version\": \"03\",\n",
    "    \"root_dir\": \"../../projects/particle/model\",\n",
    "    \"dirs\": {\n",
    "        \"weights\": \"training/weights\",\n",
    "        \"history\": \"training/history\",\n",
    "        \"metric\": \"result/metric\",\n",
    "        \"model\": \"result/model\",\n",
    "        \"predict\": \"result/predict\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"inputs\": inputs,\n",
    "        \"outputs\": outputs,\n",
    "        \"lr\": 0.0001,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": 300,\n",
    "        \"window_size\": in_time_step,\n",
    "        \"offset\": offset,\n",
    "        \"loss\": \"MSE\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"moving_average_window\": 20,\n",
    "        \"moving_average_method\": 'mean',\n",
    "        \"train\": 0.60,\n",
    "        \"validation\": 0.15,\n",
    "        \"test\": 0.25,\n",
    "        \"dates\": [\n",
    "            {\"start\": \"2022-05-07 09:40\", \"end\": \"2022-05-17 08:38\"},\n",
    "            {\"start\": \"2022-05-17 11:25\", \"end\": \"2022-05-30 23:26\"},\n",
    "            {\"start\": \"2022-06-01 22:40\", \"end\": \"2022-07-02 07:00\"},\n",
    "            {\"start\": \"2022-07-02 16:40\", \"end\": \"2022-07-09 07:13\"},\n",
    "            {\"start\": \"2022-07-09 14:30\", \"end\": \"2022-07-12 10:00\"},\n",
    "            {\"start\": \"2022-07-25 12:00\", \"end\": \"2022-08-01 10:00\"},\n",
    "            {\"start\": \"2022-08-03 09:00\", \"end\": \"2022-08-11 22:18\"},\n",
    "            {\"start\": \"2022-08-12 12:14\", \"end\": \"2022-08-20 00:00\"},\n",
    "            {\"start\": \"2022-08-20 09:38\", \"end\": \"2022-09-01 00:00\"},\n",
    "        ],\n",
    "        \"meta\": None\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6883f-cd2b-43cb-9f2d-5638585bfe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = (\n",
    "    config[\"root_dir\"] + \"/\" + config[\"name\"] + \"_\" + config[\"version\"]\n",
    ")\n",
    "\n",
    "weights_dir = config[\"dirs\"][\"weights\"]\n",
    "history_dir = config[\"dirs\"][\"history\"]\n",
    "predict_dir = config[\"dirs\"][\"predict\"]\n",
    "model_dir = config[\"dirs\"][\"model\"]\n",
    "metric_dir = config[\"dirs\"][\"metric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21477b-f15a-4125-a74f-ed4e9100213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def create_folder(path):\n",
    "    if os.path.exists(path):\n",
    "        cmd = input(f'Folder name `{path}` already exsists. You mean overwrite?[Y/n]')\n",
    "        if cmd == 'Y' or cmd == 'y':\n",
    "            shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "create_folder(root_dir + \"/\" + weights_dir)\n",
    "create_folder(root_dir + \"/\" + history_dir)\n",
    "create_folder(root_dir + \"/\" + predict_dir)\n",
    "create_folder(root_dir + \"/\" + model_dir)\n",
    "create_folder(root_dir + \"/\" + metric_dir)\n",
    "\n",
    "with open(f\"{root_dir}/config.json\", \"w\") as outfile:\n",
    "    json.dump(config, outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f78e8d-a148-4450-8dcd-1de9cf1a2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('../../storage/particle/weather.csv', index_col='DATE', parse_dates=True)[['TEMPERATURE', 'WIND_DEG', 'WIND_SPEED', 'HUMIDITY']]\n",
    "weather_df['WIND_DEG'] = np.sin(weather_df['WIND_DEG'].values * np.pi / 180)\n",
    "\n",
    "df_org = dh.load_data(\"../../storage/particle/data.csv\")\n",
    "df_org = dh.add_pm_diff(df_org)\n",
    "\n",
    "excludes = ['PERSON_NUMBER', 'AIR_PURIFIER', 'AIR_CONDITIONER', 'WINDOW', 'DOOR']\n",
    "df = dh.apply_moving_average(pd.concat([df_org, weather_df], axis=1), \n",
    "                             window=config['data']['moving_average_window'], \n",
    "                             method=config['data']['moving_average_method'], \n",
    "                             excludes=excludes, \n",
    "                             min_periods=1)\n",
    "df = pd.concat([df, df_org[excludes]], axis=1)\n",
    "df[excludes] = df[excludes].fillna(method='ffill')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "dfs = dh.trim_df(df, config['data']['dates'])\n",
    "val_size = config['data']['validation']\n",
    "test_size = config['data']['test']\n",
    "\n",
    "train_dfs, val_dfs, test_dfs = dh.train_test_split_df(dfs, val_size, test_size)\n",
    "meta_df = pd.concat(train_dfs).describe()\n",
    "meta_df.to_csv(f'{root_dir}/meta.csv', index_label='component')\n",
    "config['data']['meta'] = f'{root_dir}/meta.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d4430-0e2e-4ece-a7c4-dab54e9d2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(_dfs, in_time_step):\n",
    "    return dh.dfs_to_dataset(_dfs, meta_df, inputs, outputs, in_time_step=in_time_step, out_time_step=out_time_step, offset=offset, excludes=outputs)\n",
    "\n",
    "win_size = config['model']['window_size']\n",
    "X_train, y_train = to_dataset(train_dfs, win_size)\n",
    "X_val, y_val = to_dataset(val_dfs, win_size)\n",
    "X_test, y_test = to_dataset(test_dfs, win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba958c-bb6a-44ca-9092-cdb23eec8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=10, mode=\"min\", verbose=1, min_lr=1e-6\n",
    ")\n",
    "ely_cb = EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\", verbose=1)\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=root_dir\n",
    "    + \"/\"\n",
    "    + config[\"dirs\"][\"weights\"]\n",
    "    + \"/e{epoch:02d}-v{val_loss:.2f}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    "    period=1,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9c2f5-a0d9-4428-81b3-0c67a27470ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    LSTM,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling1D,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    MaxPooling1D,\n",
    "    Attention,\n",
    "    Permute,\n",
    ")\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input_tensor = Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = Conv1D(32, kernel_size=3, kernel_initializer='he_uniform', activation='relu', strides=1, padding='same')(input_tensor)\n",
    "    x = MaxPooling1D(pool_size=3, strides=None)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, kernel_initializer='he_uniform', activation='relu')(x)\n",
    "    # x = Dropout(0.5)(x)\n",
    "    output = Dense(y_train.shape[2], kernel_initializer='he_uniform', activation=\"relu\", name=\"output\")(x)\n",
    "\n",
    "    _model = Model(\n",
    "        inputs=input_tensor,\n",
    "        outputs=output,\n",
    "        name=f'{config[\"name\"].lower()}_v{config[\"version\"]}',\n",
    "    )\n",
    "\n",
    "    _model.compile(\n",
    "        optimizer=Adam(learning_rate=config[\"model\"][\"lr\"]),\n",
    "        loss=config[\"model\"][\"loss\"].lower(),\n",
    "        metrics=RootMeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return _model\n",
    "\n",
    "\n",
    "model = build_model((X_train.shape[1], X_train.shape[2]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b2afe-1abb-47dc-898e-dcbc23d2ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    training_res = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        epochs=config[\"model\"][\"epochs\"],\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[rlr_cb, ely_cb, mcp_cb],\n",
    "    )\n",
    "    pd.DataFrame(training_res.history).to_csv(\n",
    "        root_dir + \"/\" + config[\"dirs\"][\"history\"] + \"/history.csv\", index=False\n",
    "    )\n",
    "    plt.figure(figsize=(28, 10))\n",
    "    plt.plot(training_res.history[\"loss\"], \"o--\", label=\"train\")\n",
    "    plt.plot(training_res.history[\"val_loss\"], \"o--\", label=\"valid\")\n",
    "    plt.xlabel(\"Epochs\", fontsize=15)\n",
    "    plt.ylabel(\"Loss - RMSE\", fontsize=15)\n",
    "    plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a8b68-d311-4941-a185-52b040ff8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(_dfs, output_scaled=False):\n",
    "    res_dfs = []\n",
    "    for _df in _dfs:\n",
    "        df_cp = _df.copy()\n",
    "        _X, _y = dh.dfs_to_dataset([df_cp], meta_df, inputs, outputs, in_time_step=in_time_step)\n",
    "        y_hat = model.predict(_X, verbose=False)\n",
    "        df_cp = df_cp.iloc[in_time_step + out_time_step + offset - 1:]\n",
    "        for idx, output in enumerate(outputs):\n",
    "            if output_scaled:\n",
    "                min_val = meta_df[output]['min']\n",
    "                max_val = meta_df[output]['max']\n",
    "                df_cp[output + '_PRED'] = y_hat[:, idx] * (max_val - min_val) + min_val\n",
    "            else:\n",
    "                df_cp[output + '_PRED'] = y_hat[:, idx]\n",
    "        res_dfs.append(df_cp)\n",
    "    return pd.concat(res_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674e370-5426-4ce4-bbcd-1388fa112df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f\"{root_dir}/{weights_dir}/e32-v1.08.h5\")\n",
    "# model = tf.keras.models.load_model(\"../projects/particle/model/conv_17/result/model/conv_17.h5\")\n",
    "train_res = get_result(train_dfs)\n",
    "train_res['TYPE'] = 'train'\n",
    "val_res = get_result(val_dfs)\n",
    "val_res['TYPE'] = 'val'\n",
    "test_res = get_result(test_dfs)\n",
    "test_res['TYPE'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfb840-4cb0-40d6-8932-1b5471128677",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.plot(train_res, ['PM1_2.5_PRED', 'PM1_2.5', 'PM1_2.5_OUT', 'PM1_2.5_H_OUT', 'WINDOW', 'DOOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e556433-8039-4949-85df-952c05785eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.plot(val_res, ['PM1_2.5_PRED', 'PM1_2.5', 'PM1_2.5_OUT', 'PM1_2.5_H_OUT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415855fa-4aa7-453a-89ca-a5cb3d3a0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.plot(test_res, ['PM1_2.5_PRED', 'PM1_2.5', 'PM1_2.5_OUT', 'PM1_2.5_H_OUT', 'WINDOW', 'DOOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bc8bd-1ec6-4a52-923d-39745b8265d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax = val_res.plot.scatter(x=\"PM1_2.5\", y=\"PM1_2.5_PRED\", c=\"y\", ax=ax, fontsize=17)\n",
    "test_res.plot.scatter(x=\"PM1_2.5\", y=\"PM1_2.5_PRED\", c=\"g\", ax=ax, fontsize=17)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "\n",
    "ax.plot(lims, lims, \"r-\", linewidth=2, alpha=0.75, zorder=2)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel('REAL', fontsize=17)\n",
    "ax.set_ylabel('PRED', fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68b24f-dcc0-4760-bbfb-5f6b56d05284",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "\n",
    "cols = [\"pm1_2.5\"]\n",
    "total_res = pd.concat([train_res, val_res, test_res])\n",
    "res_dfs = [total_res, train_res, val_res, test_res]\n",
    "res_indices = [\"Total\", \"Train\", \"Validation\", \"Test\"]\n",
    "metric_funcs = [metrics.calc_r2, metrics.calc_corrcoef, metrics.calc_nmse, metrics.calc_fb, metrics.calc_b, metrics.calc_a_co]\n",
    "metrics_indices = [\"R Square\", \"Corr\", \"NMSE\", \"FB\", \"B\", \"a/C\"]\n",
    "\n",
    "\n",
    "def calc_metric(_f, _df, _col):\n",
    "    return _f(_df[_col].values, _df[_col + \"_PRED\"].values)\n",
    "\n",
    "\n",
    "for col in cols:\n",
    "    print(f\"======== {col} prediction results ========\")\n",
    "    res_dict = {\n",
    "        \"Metric\": metrics_indices,\n",
    "        \"Total\": [],\n",
    "        \"Train\": [],\n",
    "        \"Validation\": [],\n",
    "        \"Test\": [],\n",
    "    }\n",
    "\n",
    "    for j, m in enumerate(metric_funcs):\n",
    "        for i, rd in enumerate(res_dfs):\n",
    "            s = calc_metric(m, rd, col.upper())\n",
    "            res_dict[res_indices[i]].append(s)\n",
    "\n",
    "    r_df = pd.DataFrame(res_dict)\n",
    "    print(r_df)\n",
    "    print()\n",
    "    if save:\n",
    "        r_df.to_csv(\n",
    "            f'{root_dir}/{config[\"dirs\"][\"metric\"]}/result_{col}.csv',\n",
    "            index=False,\n",
    "            float_format=\"%.3f\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe65902-ca3d-457f-ae34-1f419fca475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    total_res.to_csv(\n",
    "        root_dir + \"/\" + config[\"dirs\"][\"predict\"] + \"/predict.csv\",\n",
    "        index_label=\"DATE\",\n",
    "    )\n",
    "\n",
    "    model.save(\n",
    "        root_dir\n",
    "        + \"/\"\n",
    "        + config[\"dirs\"][\"model\"]\n",
    "        + f'/{config[\"name\"].lower()}_{config[\"version\"]}.h5'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ffe9f0-d833-4567-a72d-1d7ecc607653",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52843349-04a1-4ed1-a378-e89e3b83c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "mse_data = np.zeros((len(inputs) + 1, len(outputs)))\n",
    "\n",
    "for o_idx, o in enumerate(outputs):\n",
    "    mse_data[0, o_idx] = metrics.calc_mse((pd.concat([val_res, test_res]))[o].values, (pd.concat([val_res, test_res]))[o+'_PRED'].values)\n",
    "\n",
    "for f_idx, feat in enumerate(inputs):\n",
    "    print(f'[INFO] Feature name `{feat}` starts')\n",
    "    cp_df = df.copy()\n",
    "    cp_df[feat] = np.random.permutation(cp_df[feat].values)\n",
    "    p_dfs = dh.trim_df(cp_df, config[\"data\"][\"dates\"])\n",
    "\n",
    "    train_dfs, val_dfs, test_dfs = dh.train_test_split_df(p_dfs, val_size, test_size)\n",
    "    X, y = to_dataset(val_dfs + test_dfs, win_size)\n",
    "    y_hat = model.predict(X)\n",
    "    for o_idx, o in enumerate(outputs):\n",
    "        mse_data[f_idx + 1, o_idx] = metrics.calc_mse(y[:, 0, o_idx], y_hat[:, o_idx])\n",
    "        \n",
    "mse_df = pd.DataFrame(mse_data, index=['Base']+inputs, columns=outputs)\n",
    "\n",
    "for feat_idx in mse_df.index:\n",
    "    for o in outputs:\n",
    "        mse_df.loc[feat_idx, o + '_DIFF'] = mse_df.loc[feat_idx, o] - mse_df.loc['Base', o]\n",
    "        mse_df.loc[feat_idx, o + '_INC'] = (mse_df.loc[feat_idx, o + '_DIFF']) / mse_df.loc['Base', o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944462a9-b9bb-4440-89ce-245b58bb242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse(mse_df_in, title):\n",
    "    sorted_df = mse_df_in['PM1_2.5_INC'].copy()\n",
    "    sorted_df = sorted_df.sort_values(ascending=False)\n",
    "    sorted_df = sorted_df * 100\n",
    "    ax = sorted_df.plot(kind='bar', figsize=(18, 10))\n",
    "    ax.set_ylabel('Prediction Error(%)', fontsize=17)\n",
    "    ax.set_xlabel('Feature', fontsize=17)\n",
    "    ax.set_title(title, fontsize=22)\n",
    "    ax.set_xticklabels(labels=sorted_df.index, rotation=45)\n",
    "    ax.legend(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad7d46-41d9-4cd2-8fa2-0dcb5b7b9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_df, 'Feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad885c8-e634-4fee-b9b8-23d3390ce930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
