{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0f37c-81af-4b3f-9b4b-5ab45fc071e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "sys.path.append(\"./scripts/particles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775d93e-6262-4742-9cc4-dfd234b0670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handler as dh\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4ce48-23a6-4ba9-b9c1-6f64d56d8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89c1c6-7349-4c85-892a-cd22bc346f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['PM1', 'PM2.5', 'PM10']\n",
    "inputs = [\n",
    "    'PM1_2.5_OUT',\n",
    "    'PM1_2.5_H_OUT',\n",
    "    'PM2.5_OUT',\n",
    "    'PM2.5_H_OUT',\n",
    "    'PM2.5_10_OUT',\n",
    "    'PM2.5_10_H_OUT',\n",
    "    'PERSON_NUMBER',\n",
    "    'AIR_PURIFIER',\n",
    "    'WINDOW',\n",
    "    'AIR_CONDITIONER',\n",
    "    'DOOR',\n",
    "    # 'TEMPERATURE',\n",
    "    # 'WIND_SPEED',\n",
    "    'WIND_DEG',\n",
    "    'HUMIDITY'\n",
    "]\n",
    "\n",
    "in_time_step = 60\n",
    "offset = 1\n",
    "out_time_step = 1\n",
    "batch_size = 64\n",
    "\n",
    "config = {\n",
    "    \"name\": \"conv\",\n",
    "    \"description\": \"High quality prediction interval model with conv1d\",\n",
    "    \"version\": \"hq07\",\n",
    "    \"root_dir\": \"../projects/particle/model\",\n",
    "    \"dirs\": {\n",
    "        \"weights\": \"training/weights\",\n",
    "        \"history\": \"training/history\",\n",
    "        \"metric\": \"result/metric\",\n",
    "        \"model\": \"result/model\",\n",
    "        \"predict\": \"result/predict\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"lr\": 0.0001,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": 300,\n",
    "        \"window_size\": in_time_step,\n",
    "        \"offset\": offset,\n",
    "        \"loss\": \"MSE\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"moving_average_window\": 20,\n",
    "        \"moving_average_method\": 'mean',\n",
    "        \"train\": 0.60,\n",
    "        \"validation\": 0.15,\n",
    "        \"test\": 0.25,\n",
    "        \"dates\": [\n",
    "            {\"start\": \"2022-05-07 09:40\", \"end\": \"2022-05-17 08:38\"},\n",
    "            {\"start\": \"2022-05-17 11:25\", \"end\": \"2022-05-30 23:26\"},\n",
    "            {\"start\": \"2022-06-01 22:40\", \"end\": \"2022-07-02 07:00\"},\n",
    "            {\"start\": \"2022-07-02 16:40\", \"end\": \"2022-07-09 07:13\"},\n",
    "            {\"start\": \"2022-07-09 14:30\", \"end\": \"2022-07-12 10:00\"},\n",
    "            {\"start\": \"2022-07-25 12:00\", \"end\": \"2022-08-01 10:00\"},\n",
    "            {\"start\": \"2022-08-03 09:00\", \"end\": \"2022-08-11 22:18\"},\n",
    "            {\"start\": \"2022-08-12 12:14\", \"end\": \"2022-08-20 00:00\"},\n",
    "            {\"start\": \"2022-08-20 09:38\", \"end\": \"2022-09-01 00:00\"},\n",
    "        ],\n",
    "        \"meta\": None\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6883f-cd2b-43cb-9f2d-5638585bfe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = (\n",
    "    config[\"root_dir\"] + \"/\" + config[\"name\"] + \"_\" + config[\"version\"]\n",
    ")\n",
    "\n",
    "weights_dir = config[\"dirs\"][\"weights\"]\n",
    "history_dir = config[\"dirs\"][\"history\"]\n",
    "predict_dir = config[\"dirs\"][\"predict\"]\n",
    "model_dir = config[\"dirs\"][\"model\"]\n",
    "metric_dir = config[\"dirs\"][\"metric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21477b-f15a-4125-a74f-ed4e9100213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def create_folder(path):\n",
    "    if os.path.exists(path):\n",
    "        cmd = input(f'Folder name `{path}` already exsists. You mean overwrite?[Y/n]')\n",
    "        if cmd == 'Y' or cmd == 'y':\n",
    "            shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "create_folder(root_dir + \"/\" + weights_dir)\n",
    "create_folder(root_dir + \"/\" + history_dir)\n",
    "create_folder(root_dir + \"/\" + predict_dir)\n",
    "create_folder(root_dir + \"/\" + model_dir)\n",
    "create_folder(root_dir + \"/\" + metric_dir)\n",
    "\n",
    "with open(f\"{root_dir}/config.json\", \"w\") as outfile:\n",
    "    json.dump(config, outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f78e8d-a148-4450-8dcd-1de9cf1a2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('../storage/particle/weather.csv', index_col='DATE', parse_dates=True)[['TEMPERATURE', 'WIND_DEG', 'WIND_SPEED', 'HUMIDITY']]\n",
    "weather_df['WIND_DEG'] = np.sin(weather_df['WIND_DEG'].values * np.pi / 180)\n",
    "\n",
    "df_org = dh.load_data(\"../storage/particle/data.csv\")\n",
    "df_org = dh.add_pm_diff(df_org)\n",
    "\n",
    "excludes = ['PERSON_NUMBER', 'AIR_PURIFIER', 'AIR_CONDITIONER', 'WINDOW', 'DOOR']\n",
    "df = dh.apply_moving_average(pd.concat([df_org, weather_df], axis=1), \n",
    "                             window=config['data']['moving_average_window'], \n",
    "                             method=config['data']['moving_average_method'], \n",
    "                             excludes=excludes, \n",
    "                             min_periods=1)\n",
    "df = pd.concat([df, df_org[excludes]], axis=1)\n",
    "df[excludes] = df[excludes].fillna(method='ffill')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "dfs = dh.trim_df(df, config['data']['dates'])\n",
    "val_size = config['data']['validation']\n",
    "test_size = config['data']['test']\n",
    "\n",
    "train_dfs, val_dfs, test_dfs = dh.train_test_split_df(dfs, val_size, test_size)\n",
    "meta_df = pd.concat(train_dfs).describe()\n",
    "meta_df.to_csv(f'{root_dir}/meta.csv', index_label='component')\n",
    "config['data']['meta'] = f'{root_dir}/meta.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d4430-0e2e-4ece-a7c4-dab54e9d2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(_dfs, in_time_step):\n",
    "    return dh.dfs_to_dataset(_dfs, meta_df, inputs, outputs, in_time_step=in_time_step, out_time_step=out_time_step, offset=offset, excludes=outputs)\n",
    "\n",
    "win_size = config['model']['window_size']\n",
    "X_train, y_train = to_dataset(train_dfs, win_size)\n",
    "X_val, y_val = to_dataset(val_dfs, win_size)\n",
    "X_test, y_test = to_dataset(test_dfs, win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba958c-bb6a-44ca-9092-cdb23eec8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=30, mode=\"min\", verbose=1, min_lr=1e-6\n",
    ")\n",
    "ely_cb = EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\", verbose=1)\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=root_dir\n",
    "    + \"/\"\n",
    "    + config[\"dirs\"][\"weights\"]\n",
    "    + \"/e{epoch:02d}-v{val_loss:.2f}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    "    period=1,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a6032-81e2-4368-833e-8c91cf9a14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = 0.01\n",
    "lmbd_2 = 0.7\n",
    "alpha = 0.05\n",
    "\n",
    "soften = 300.\n",
    "loss_type = 'soft' # soft or hard\n",
    "\n",
    "def qd_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :, 0]\n",
    "    y_u = y_pred[:, :, 0]\n",
    "    y_l = y_pred[:, :, 1]\n",
    "\n",
    "    k_u = None\n",
    "    k_l = None\n",
    "    if loss_type == 'soft':\n",
    "        k_u = tf.sigmoid(soften * (y_u - y_true))\n",
    "        k_l = tf.sigmoid(soften * (y_true - y_l))\n",
    "    elif loss_type == 'hard':\n",
    "        k_u = tf.maximum(0., tf.sign(y_u - y_true))\n",
    "        k_l = tf.maximum(0., tf.sign(y_true - y_l))\n",
    "\n",
    "    k = tf.multiply(k_u, k_l)\n",
    "    mpiw = tf.reduce_sum(tf.multiply(y_u - y_l, k)) / tf.maximum(tf.reduce_sum(k), 1e-12)\n",
    "    picp = tf.reduce_mean(k)\n",
    "    # mpiw_2 = tf.sqrt(tf.reduce_mean(tf.square(tf.multiply(y_u - y_true, y_true - y_l))))\n",
    "    rmsqrt_u = tf.sqrt(tf.reduce_mean(tf.square(y_u - y_true)))\n",
    "    rmsqrt_l = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_l)))\n",
    "    rmsqrt = (rmsqrt_u + rmsqrt_l) / 2\n",
    "\n",
    "    loss = mpiw + rmsqrt * lmbd_2 + lmbd * batch_size * tf.square(tf.maximum(0., (1 - alpha) - picp)) / (alpha * (1 - alpha))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657a166-7445-4997-a102-b8ec8671baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = 0.03\n",
    "gamma = 0.7\n",
    "alpha = 0.05\n",
    "\n",
    "soften = 300.\n",
    "loss_type = 'soft' # soft or hard\n",
    "\n",
    "def qd_loss_v2(y_true, y_pred):\n",
    "    y_true = y_true[:, :, 0]\n",
    "    y_u = y_pred[:, :, 0]\n",
    "    y_l = y_pred[:, :, 1]\n",
    "\n",
    "    k_u = None\n",
    "    k_l = None\n",
    "    if loss_type == 'soft':\n",
    "        k_u = tf.sigmoid(soften * (y_u - y_true))\n",
    "        k_l = tf.sigmoid(soften * (y_true - y_l))\n",
    "    elif loss_type == 'hard':\n",
    "        k_u = tf.maximum(0., tf.sign(y_u - y_true))\n",
    "        k_l = tf.maximum(0., tf.sign(y_true - y_l))\n",
    "\n",
    "    k = tf.multiply(k_u, k_l)\n",
    "    mpiw = tf.reduce_sum(tf.multiply(y_u - y_l, k)) / tf.maximum(tf.reduce_sum(k), 1e-12)\n",
    "    picp = tf.reduce_mean(k)\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(y_u - y))) + tf.sqrt(tf.reduce_mean(tf.square(y - y_l)))\n",
    "\n",
    "    loss = mpiw + gamma * tf.sqrt(tf.reduce_mean(tf.square(tf.multiply(y_u - y_true, y_true - y_l)))) + lmbd * batch_size * tf.square(tf.maximum(0., (1 - alpha) - picp)) / (alpha * (1 - alpha))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120afacd-7739-4433-b474-5bef38e4f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = 0.015\n",
    "gamma = 0.4\n",
    "alpha = 0.05\n",
    "\n",
    "soften = 300.\n",
    "loss_type = 'soft' # soft or hard\n",
    "\n",
    "def qd_loss_v2(y_true, y_pred):\n",
    "    y_true = y_true[:, :, 0]\n",
    "    y_u = y_pred[:, :, 0]\n",
    "    y_l = y_pred[:, :, 1]\n",
    "\n",
    "    k_u = None\n",
    "    k_l = None\n",
    "    if loss_type == 'soft':\n",
    "        k_u = tf.sigmoid(soften * (y_u - y_true))\n",
    "        k_l = tf.sigmoid(soften * (y_true - y_l))\n",
    "    elif loss_type == 'hard':\n",
    "        k_u = tf.maximum(0., tf.sign(y_u - y_true))\n",
    "        k_l = tf.maximum(0., tf.sign(y_true - y_l))\n",
    "\n",
    "    k = tf.multiply(k_u, k_l)\n",
    "    mpiw = tf.reduce_sum(tf.multiply(y_u - y_l, k)) / tf.maximum(tf.reduce_sum(k), 1e-12)\n",
    "    picp = tf.reduce_mean(k)\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(y_u - y_true))) + tf.sqrt(tf.reduce_mean(tf.square(y_true - y_l)))\n",
    "\n",
    "    loss = mpiw + gamma * rmse + lmbd * batch_size * tf.square(tf.maximum(0., (1 - alpha) - picp)) / (alpha * (1 - alpha))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a38bf2-da81-4cd8-b712-26af4ba040a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_org = keras.models.load_model('../projects/particle/model/conv_19/result/model/conv_19.h5')\n",
    "model_org.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9c2f5-a0d9-4428-81b3-0c67a27470ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    LSTM,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalMaxPooling1D,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    MaxPooling1D,\n",
    "    Reshape\n",
    ")\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model_inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = model_org(model_inputs)\n",
    "output = Dense(\n",
    "    y_train.shape[2]*2, \n",
    "    kernel_initializer=keras.initializers.Constant(value=[[1,1,0,0,0,0], [0,0,1,1,0,0], [0,0,0,0,1,1]]),\n",
    "    name='range_output',\n",
    "    activation='relu',\n",
    "    bias_initializer=keras.initializers.Constant(value=[10.,-10.]*3),\n",
    ")(x)\n",
    "output = Reshape((-1, 2))(output)\n",
    "\n",
    "model = Model(\n",
    "    inputs=model_inputs,\n",
    "    outputs=output,\n",
    "    name=f'{config[\"name\"].lower()}_v{config[\"version\"]}',\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=config[\"model\"][\"lr\"]),\n",
    "    loss=qd_loss_v2,\n",
    "    metrics=RootMeanSquaredError(),\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b2afe-1abb-47dc-898e-dcbc23d2ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    training_res = model.fit(\n",
    "        x=X_train,\n",
    "        y=np.stack((y_train[:, 0, :], y_train[:, 0, :]), axis=2),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        epochs=100,\n",
    "        validation_data=(X_val, np.stack((y_val[:, 0, :], y_val[:, 0, :]), axis=2)),\n",
    "        callbacks=[rlr_cb, ely_cb, mcp_cb],\n",
    "    )\n",
    "    pd.DataFrame(training_res.history).to_csv(\n",
    "        root_dir + \"/\" + config[\"dirs\"][\"history\"] + \"/history.csv\", index=False\n",
    "    )\n",
    "    plt.figure(figsize=(28, 10))\n",
    "    plt.plot(training_res.history[\"loss\"], \"o--\", label=\"train\")\n",
    "    plt.plot(training_res.history[\"val_loss\"], \"o--\", label=\"valid\")\n",
    "    plt.xlabel(\"Epochs\", fontsize=15)\n",
    "    plt.ylabel(\"Loss - RMSE\", fontsize=15)\n",
    "    plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c062d71-732f-4062-9c3b-4f462312b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f\"{root_dir}/{weights_dir}/e50-v33.82.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a8b68-d311-4941-a185-52b040ff8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(_dfs, output_scaled=False):\n",
    "    res_dfs = []\n",
    "    for _df in _dfs:\n",
    "        df_cp = _df.copy()\n",
    "        _X, _y = dh.dfs_to_dataset([df_cp], meta_df, inputs, outputs, in_time_step=in_time_step)\n",
    "        y_hat = model.predict(_X, verbose=False)\n",
    "        org_y_hat = model_org.predict(_X, verbose=False)\n",
    "        df_cp = df_cp.iloc[in_time_step + out_time_step + offset - 1:]\n",
    "        for idx, output in enumerate(outputs):\n",
    "            if output_scaled:\n",
    "                min_val = meta_df[output]['min']\n",
    "                max_val = meta_df[output]['max']\n",
    "                df_cp[output + '_U_PRED'] = y_hat[:, idx, 0] * (max_val - min_val) + min_val\n",
    "                df_cp[output + '_L_PRED'] = y_hat[:, idx, 1] * (max_val - min_val) + min_val\n",
    "                df_cp[output + '_PRED'] = org_y_hat[:, idx] * (max_val - min_val) + min_val\n",
    "            else:\n",
    "                df_cp[output + '_U_PRED'] = y_hat[:, idx, 0]\n",
    "                df_cp[output + '_L_PRED'] = y_hat[:, idx, 1]\n",
    "                df_cp[output + '_PRED'] = org_y_hat[:, idx]\n",
    "        res_dfs.append(df_cp)\n",
    "    return pd.concat(res_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674e370-5426-4ce4-bbcd-1388fa112df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(f\"{root_dir}/{weights_dir}/e45-v15.40.h5\")\n",
    "\n",
    "# model = tf.keras.models.load_model(\"../projects/particle/model/conv_hq06/result/model/conv_hq06.h5\", compile=False)\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=config[\"model\"][\"lr\"]),\n",
    "#     loss=qd_loss_v2,\n",
    "#     metrics=RootMeanSquaredError(),\n",
    "# )\n",
    "\n",
    "train_res = get_result(train_dfs)\n",
    "train_res['TYPE'] = 'train'\n",
    "val_res = get_result(val_dfs)\n",
    "val_res['TYPE'] = 'val'\n",
    "test_res = get_result(test_dfs)\n",
    "test_res['TYPE'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb1022-5ed0-404b-bd02-afe8258f15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609c4dc-b7ea-485a-8aaf-3eccef3dcd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.load_weights('project/GRU/GRU09/training/weights/e23-v17.85.h5')\n",
    "# model = tf.keras.models.load_model(\"project/GRU/GRUkt01/result/model/gru_kt01.h5\")\n",
    "# train_res = get_result_df(model, train_dfs, meta)\n",
    "# val_res = get_result_df(model, val_dfs, meta)\n",
    "# test_res = get_result_df(model, test_dfs, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f72690-2a03-4c88-b0f5-134dbe4d9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_res = pd.concat([train_res, val_res, test_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924078e-980f-4647-935d-4e333128c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_res.resample('1T').mean().plot(y=['PM2.5'], figsize=(30, 8))\n",
    "ax.fill_between(\n",
    "    train_res.index,\n",
    "    train_res['PM2.5_L_PRED'],\n",
    "    train_res['PM2.5_U_PRED'],\n",
    "    facecolor=\"green\",\n",
    "    alpha=0.2,\n",
    "    interpolate=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533a1bd-6791-4ca7-a7a6-7ec104b3569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = val_res[(val_res.index >=pd.to_datetime('2022-07-25')) & (val_res.index <=pd.to_datetime('2022-08-01'))].resample('1T').mean().plot(y=['PM2.5'], figsize=(30, 8))\n",
    "ax = val_res.resample('1T').mean().plot(y=['PM2.5'], figsize=(30, 8))\n",
    "ax.fill_between(\n",
    "    val_res.index,\n",
    "    val_res['PM2.5_L_PRED'],\n",
    "    val_res['PM2.5_U_PRED'],\n",
    "    facecolor=\"green\",\n",
    "    alpha=0.2,\n",
    "    interpolate=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078bbad-e32b-45a3-9f3c-0ec5baafc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = test_res.resample('1T').mean().plot(y=['PM2.5'], figsize=(30, 8))\n",
    "ax.fill_between(\n",
    "    test_res.index,\n",
    "    test_res['PM2.5_L_PRED'],\n",
    "    test_res['PM2.5_U_PRED'],\n",
    "    facecolor=\"green\",\n",
    "    alpha=0.2,\n",
    "    interpolate=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68b24f-dcc0-4760-bbfb-5f6b56d05284",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = model.predict(X_train)\n",
    "yhat_val = model.predict(X_val)\n",
    "yhat_test = model.predict(X_test)\n",
    "\n",
    "train_loss = qd_loss_v2(np.stack((y_train[:, 0, :], y_train[:, 0, :]), axis=2), yhat_train)\n",
    "val_loss = qd_loss_v2(np.stack((y_val[:, 0, :], y_val[:, 0, :]), axis=2), yhat_val)\n",
    "test_loss = qd_loss_v2(np.stack((y_test[:, 0, :], y_test[:, 0, :]), axis=2), yhat_test)\n",
    "print(f'train loss: {train_loss:.3f}, val loss: {val_loss:.3f}, test loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d12de7-6000-47a1-9f45-462ac57789df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tmps = [y_train, y_val, y_test]\n",
    "yhat_tmps = [yhat_train, yhat_val, yhat_test]\n",
    "\n",
    "for y_tmp, yhat_tmp in zip(y_tmps, yhat_tmps):\n",
    "    count = 0\n",
    "    for i in range(len(yhat_tmp)):\n",
    "        for j in range(3):\n",
    "            if y_tmp[i, 0, j] >= yhat_tmp[i, j, 1] and y_tmp[i, 0, j] <= yhat_tmp[i, j, 0]:\n",
    "                count += 1\n",
    "\n",
    "    mpiw = np.sum(yhat_tmp[:, :, 0] - yhat_tmp[:, :, 1]) / (len(yhat_tmp) * 3)\n",
    "    print(f'Captured: {(count / 3) / len(y_tmp):.3f}, MPIW: {mpiw:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe65902-ca3d-457f-ae34-1f419fca475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\n",
    "    root_dir\n",
    "    + \"/\"\n",
    "    + config[\"dirs\"][\"model\"]\n",
    "    + f'/{config[\"name\"].lower()}_{config[\"version\"]}.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecdf04-9339-443c-8167-42006999e263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
