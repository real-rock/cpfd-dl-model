{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa439a-7396-4ddc-80d5-81a775ec3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDOE import lhs\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee4275-4071-4f83-a256-fc3a1ce0ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts/particles/\")\n",
    "\n",
    "import data_handler as dh\n",
    "import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b61dd5-8712-4277-a6a5-e156f8621785",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['PM1', 'PM2.5', 'PM10']\n",
    "inputs = [\n",
    "    'PM1_2.5_OUT', \n",
    "    'PM1_2.5_H_OUT',\n",
    "    'PM2.5_OUT', \n",
    "    'PM2.5_H_OUT',\n",
    "    'PM2.5_10_OUT',\n",
    "    'PM2.5_10_H_OUT',\n",
    "    'PERSON_NUMBER',\n",
    "    'AIR_PURIFIER',\n",
    "    'WINDOW',\n",
    "    'AIR_CONDITIONER',\n",
    "    'DOOR',\n",
    "    # 'TEMPERATURE',\n",
    "    # 'WIND_SPEED',\n",
    "    'WIND_DEG',\n",
    "    'HUMIDITY'\n",
    "]\n",
    "\n",
    "dates = [\n",
    "    {\"start\": \"2022-05-07 09:40\", \"end\": \"2022-05-17 08:38\"},\n",
    "    {\"start\": \"2022-05-17 11:25\", \"end\": \"2022-05-30 23:26\"},\n",
    "    {\"start\": \"2022-06-01 22:40\", \"end\": \"2022-07-02 07:00\"},\n",
    "    {\"start\": \"2022-07-02 16:40\", \"end\": \"2022-07-09 07:13\"},\n",
    "    {\"start\": \"2022-07-09 14:30\", \"end\": \"2022-07-12 10:00\"},\n",
    "    {\"start\": \"2022-07-25 12:00\", \"end\": \"2022-08-01 10:00\"},\n",
    "    {\"start\": \"2022-08-03 09:00\", \"end\": \"2022-08-11 22:18\"},\n",
    "    {\"start\": \"2022-08-12 12:14\", \"end\": \"2022-08-20 00:00\"},\n",
    "    {\"start\": \"2022-08-20 09:38\", \"end\": \"2022-09-01 00:00\"},\n",
    "]\n",
    "\n",
    "moving_average_window = 20\n",
    "moving_average_method = 'mean'\n",
    "val_size = 0.15\n",
    "test_size = 0.25\n",
    "train_size = 1 - val_size - test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aad361-318d-403a-aa86-e35e5a88ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('../../storage/particle/weather.csv', index_col='DATE', parse_dates=True)[['TEMPERATURE', 'WIND_DEG', 'WIND_SPEED', 'HUMIDITY']]\n",
    "weather_df['WIND_DEG'] = np.sin(weather_df['WIND_DEG'].values * np.pi / 180)\n",
    "\n",
    "df_org = dh.load_data(\"../../storage/particle/data.csv\")\n",
    "df_org = dh.add_pm_diff(df_org)\n",
    "\n",
    "excludes = ['PERSON_NUMBER', 'AIR_PURIFIER', 'AIR_CONDITIONER', 'WINDOW', 'DOOR']\n",
    "df = dh.apply_moving_average(pd.concat([df_org, weather_df], axis=1), \n",
    "                             window=moving_average_window, \n",
    "                             method=moving_average_method, \n",
    "                             excludes=excludes)\n",
    "df = pd.concat([df, df_org[excludes]], axis=1)\n",
    "df[excludes] = df[excludes].fillna(method='ffill')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "dfs = dh.trim_df(df, dates)\n",
    "train_dfs, val_dfs, test_dfs = dh.train_test_split_df(dfs, val_size, test_size)\n",
    "meta_df = pd.concat(train_dfs).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56520655-2665-4982-a78d-1531c991aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = df[(df.index >= pd.to_datetime('2022-07-09 23:00')) & (df.index <= pd.to_datetime('2022-07-12'))].resample('T').first().fillna(value=np.nan).plot(kind='line', y=['PM2.5', 'PM2.5_OUT', 'PM2.5_H_OUT'], figsize=(18, 12), fontsize=17)\n",
    "# ax.set_ylabel('PM2.5 $\\mu g m^3$', fontsize=17)\n",
    "# ax.set_xlabel('Date', fontsize=17)\n",
    "# ax.legend(fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b523d-c1d2-4eee-8c55-f3157a72a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_params = {\n",
    "    \"window_size\": [5, 12, 16, 30, 60],\n",
    "    \"pool_size\": [x for x in range(2, 6)],\n",
    "    \"pool_strides\": [x for x in range(1, 4)],\n",
    "    \"dense\": {\n",
    "        \"units\": [x*32+16 for x in range(8)],\n",
    "        \"dropout\": np.arange(0, 0.5+0.05, 0.05),\n",
    "        \"leaky_relu\": np.arange(0, 0.5+0.05, 0.05),\n",
    "    },\n",
    "    \"batch_size\": [x*32+32 for x in range(8)],\n",
    "    \"lr\": [0.001, 0.0001, 0.00001],\n",
    "}\n",
    "\n",
    "conv_params = {\n",
    "    \"conv_0\": {\n",
    "        \"filters\": [x*32+16 for x in range(8)],\n",
    "        \"kernel_size\": [x*2+3 for x in range(3)],\n",
    "        \"strides\": [x+1 for x in range(3)],\n",
    "    },\n",
    "    \"conv_1\": {\n",
    "        \"filters\": [None]+[x*32+16 for x in range(8)],\n",
    "        \"kernel_size\": [x*2+3 for x in range(3)],\n",
    "        \"strides\": [x+1 for x in range(3)],\n",
    "    },\n",
    "}\n",
    "\n",
    "rnn_params = {\n",
    "    \"conv_0\": {\n",
    "        \"activated\": [True, False],\n",
    "        \"filters\": [None]+[x*32+16 for x in range(8)],\n",
    "        \"kernel_size\": [x*2+3 for x in range(3)],\n",
    "        \"strides\": [x+1 for x in range(3)],\n",
    "    },\n",
    "    \"rnn_0\": {\n",
    "        \"layer\": ['naive', 'lstm', 'gru'],\n",
    "        \"units\": [x*32+16 for x in range(8)],\n",
    "        \"dropout\": np.arange(0, 0.5+0.05, 0.05),\n",
    "    },\n",
    "    \"rnn_1\": {\n",
    "        \"layer\": ['naive', 'lstm', 'gru'],\n",
    "        \"units\": [None]+[x*32+16 for x in range(8)],\n",
    "        \"dropout\": np.arange(0, 0.5+0.05, 0.05),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e1b9c-272e-48ee-af09-a7e8b11fbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_val = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "basic_params = {\n",
    "    \"window_size\": [12, 60],\n",
    "    \"pool_size\": [2, 6],\n",
    "    \"pool_strides\": [1, 4],\n",
    "    \"dense\": {\n",
    "        \"units\": [32, 256],\n",
    "        \"dropout\": [0, 0.5],\n",
    "        \"leaky_relu\": [0, 0.5],\n",
    "    },\n",
    "    \"batch_size\": [32, 256],\n",
    "    \"lr\": [0, 2],\n",
    "}\n",
    "\n",
    "conv_params = {\n",
    "    \"conv\": {\n",
    "        \"filters\": [32, 256],\n",
    "        \"kernel_size\": [3, 7],\n",
    "        \"strides\": [0, 3],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ac9fe-faaa-4c86-a49b-e8b1a4150e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = {}\n",
    "val_ds = {}\n",
    "test_ds = {}\n",
    "\n",
    "def to_dataset(_dfs, in_time_step):\n",
    "    return dh.dfs_to_dataset(_dfs, meta_df, inputs, outputs, in_time_step=in_time_step)\n",
    "\n",
    "for win_size in basic_params[\"window_size\"]:\n",
    "    train_ds[str(win_size)] = to_dataset(train_dfs, win_size)\n",
    "    val_ds[str(win_size)] = to_dataset(val_dfs, win_size)\n",
    "    test_ds[str(win_size)] = to_dataset(test_dfs, win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6aaba-ae18-4dc6-a5ba-0a7947f5e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_len(param):\n",
    "    l = 0\n",
    "    for p in param.keys():\n",
    "        if type(param[p]) == dict:\n",
    "            l += get_param_len(param[p])\n",
    "        else:\n",
    "            l += 1\n",
    "    return l\n",
    "\n",
    "def get_param_len_list(param):\n",
    "    li = []\n",
    "    for p in param.keys():\n",
    "        if type(param[p]) == dict:\n",
    "            li += get_param_len_list(param[p])\n",
    "        else:\n",
    "            li.append(len(param[p]))\n",
    "    return li\n",
    "\n",
    "def get_param_keys(param):\n",
    "    keys = []\n",
    "    for p in param.keys():\n",
    "        if type(param[p]) == dict:\n",
    "            keys += get_param_len_list(param[p])\n",
    "        else:\n",
    "            keys.append(p)\n",
    "    return keys\n",
    "\n",
    "def get_samples(param, n_samples):\n",
    "    n_dim = get_param_len(basic_params) + get_param_len(param)\n",
    "    return lhs(n_dim, n_samples, 'maximin')\n",
    "\n",
    "def smp_to_indices(sample, param):\n",
    "    len_list = get_param_len_list(basic_params) + get_param_len_list(param)\n",
    "    smp_cpy = np.zeros(sample.shape)\n",
    "    if len(len_list) != sample.shape[1]:\n",
    "        print('[ERROR] invalid shape')\n",
    "        return\n",
    "    for i, ll in enumerate(len_list):\n",
    "        smp_cpy[:, i] = np.floor(sample[:, i] * ll)\n",
    "    smp_cpy = np.int32(smp_cpy)\n",
    "    return smp_cpy\n",
    "\n",
    "def dict_cat(param):\n",
    "    new_param = {}\n",
    "    for key in basic_params.keys():\n",
    "        new_param[key] = basic_params[key]\n",
    "    for key in param.keys():\n",
    "        new_param[key] = param[key]\n",
    "    return new_param\n",
    "\n",
    "def get_smp_values(param, _indices):\n",
    "    val_dict = {}\n",
    "    param = dict_cat(param)\n",
    "    val_list = get_param_len_list(param)\n",
    "    i = 0\n",
    "    for p in param.keys():\n",
    "        if type(param[p]) == dict:\n",
    "            val_dict[p] = {}\n",
    "            for p2 in param[p].keys():\n",
    "                val_dict[p][p2] = param[p][p2][_indices[i]]\n",
    "                i += 1\n",
    "        else:\n",
    "            val_dict[p] = param[p][_indices[i]]\n",
    "            i += 1\n",
    "    return val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f4b94-6188-4376-abdc-4c9f33908c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_param_len(basic_params) + get_param_len(conv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b2db0-1fd0-49d9-9db3-7510a3ee1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_data = lhs(11, 10000, 'maximin')\n",
    "# cubic_data = np.load('cubic.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9fe49-66b6-4cc9-b43a-4964f0b8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../storage/particle/lhs_conv.npy', cubic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72e189-2414-427f-a641-1fba39ef8c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cubic_org = np.copy(cubic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f173f8-45b9-49b7-9f12-fa78504dd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "\n",
    "for k in basic_params.keys():\n",
    "    if type(basic_params[k]) == dict:\n",
    "        for k2 in basic_params[k]:\n",
    "            param_dict[k+'_'+k2] = basic_params[k][k2]\n",
    "    else:\n",
    "        param_dict[k] = basic_params[k]\n",
    "        \n",
    "for k in conv_params.keys():\n",
    "    if type(conv_params[k]) == dict:\n",
    "        for k2 in conv_params[k]:\n",
    "            param_dict[k+'_'+k2] = conv_params[k][k2]\n",
    "    else:\n",
    "        param_dict[k] = conv_params[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125888d1-3d04-4729-ab42-e864762e9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(param_dict.keys()):\n",
    "    p_min = param_dict[k][0]\n",
    "    p_max = param_dict[k][1]\n",
    "    cubic_data[:, i] = cubic_data[:, i]*(p_max - p_min) + p_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63697baf-0b4f-4968-ba1d-13e3acb798b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../projects/particle/lhs_opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644201f-552c-409f-8d89-26e086064b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conv_smp = get_samples(conv_params, 10000)\n",
    "# new_conv_idc = smp_to_indices(new_conv_smp, conv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52301c34-e8b7-41ce-95a5-01efd7c93caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "# conv_smp = get_samples(conv_params, 1024)\n",
    "# conv_idc = smp_to_indices(conv_smp, conv_params)\n",
    "\n",
    "# rnn_smp = get_samples(rnn_params, 1024)\n",
    "# rnn_idc = smp_to_indices(rnn_smp, rnn_params)\n",
    "\n",
    "proj_dir = f\"{root_dir}/{dt.datetime.strftime(dt.datetime.now(), '%Y-%m-%d_%H:%M')}\"\n",
    "os.makedirs(proj_dir)\n",
    "\n",
    "np.save(f'{proj_dir}/conv_idc.npy', cubic_data)\n",
    "# np.save(f'{proj_dir}/rnn_idc.npy', rnn_idc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9fdaf-7c3f-456b-b960-e1ffd8eca55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '../../projects/particle/lhs_opt/2022-09-10_11:43'\n",
    "conv_idc = np.load(f'{proj_dir}/conv_idc.npy')\n",
    "# rnn_idc = np.load(f'{proj_dir}/rnn_idc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f9ac4-aaa8-467f-9008-3b2d32fe5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    LSTM,\n",
    "    SimpleRNN,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_conv_layer(param, _input):\n",
    "    x = _input\n",
    "    for p in param.keys():\n",
    "        info = str(p).split('_')\n",
    "        layer_type = info[0]\n",
    "        if layer_type == 'conv' and param[p][\"filters\"] is not None:\n",
    "            f = param[p][\"filters\"]\n",
    "            k = param[p][\"kernel_size\"]\n",
    "            s = param[p][\"strides\"]\n",
    "            x = Conv1D(f, kernel_size=k, kernel_initializer='he_uniform', activation='relu', strides=s, padding='same')(x)\n",
    "    return x\n",
    "\n",
    "def build_rnn_layer(param, _input):\n",
    "    x = _input\n",
    "    for p in param.keys():\n",
    "        info = str(p).split('_')\n",
    "        layer_type = info[0]\n",
    "        num_type = int(info[1])\n",
    "        if layer_type == 'conv' and param[p][\"activated\"]:\n",
    "            f = param[p][\"filters\"]\n",
    "            k = param[p][\"kernel_size\"]\n",
    "            s = param[p][\"strides\"]\n",
    "            i += 3\n",
    "            x = Conv1D(f, kernel_size=k, kernel_initializer='he_uniform', activation='relu', strides=s, padding='same')(x)\n",
    "        elif layer_type == 'rnn':\n",
    "            layer = param[p][\"layer\"]\n",
    "            units = param[p][\"units\"]\n",
    "            dropout = param[p][\"dropout\"]\n",
    "            if layer == 'naive':\n",
    "                x = SimpleRNN(units=units, \n",
    "                              dropout=dropout,\n",
    "                              activation='tanh', \n",
    "                              kernel_initializer='glorot_uniform', \n",
    "                              return_sequences=True,\n",
    "                             )(x)\n",
    "            elif layer == 'lstm':\n",
    "                x = LSTM(units=units,\n",
    "                         dropout=dropout,\n",
    "                         activation='tanh', \n",
    "                         kernel_initializer='glorot_uniform', \n",
    "                         return_sequences=True,\n",
    "                        )(x)\n",
    "            elif layer == 'gru':\n",
    "                x = GRU(units=units, \n",
    "                        dropout=dropout,\n",
    "                        activation='tanh', \n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        return_sequences=True,\n",
    "                       )(x)\n",
    "    return x\n",
    "\n",
    "def model_builder(p, input_shape, output_size, layer_type='conv'):\n",
    "    input_tensor = Input(shape=input_shape, name=\"input\")\n",
    "    x = input_tensor\n",
    "    if layer_type == 'conv':\n",
    "        x = build_conv_layer(p, x)\n",
    "    elif layer_type == 'rnn':\n",
    "        x = build_rnn_layer(p, x)\n",
    "    \n",
    "    x = MaxPooling1D(pool_size=p[\"pool_size\"], strides=p[\"pool_strides\"], padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(p[\"dense\"][\"units\"], kernel_initializer='he_uniform', activation=LeakyReLU(p[\"dense\"][\"leaky_relu\"]))(x)\n",
    "    x = Dropout(p[\"dense\"][\"dropout\"])(x)\n",
    "    output = Dense(output_size, kernel_initializer='he_uniform', activation=\"relu\", name=\"output\")(x)\n",
    "\n",
    "    _model = Model(\n",
    "        inputs=input_tensor,\n",
    "        outputs=output,\n",
    "        name='test',\n",
    "    )\n",
    "\n",
    "    _model.compile(\n",
    "        optimizer=Adam(learning_rate=p[\"lr\"]),\n",
    "        loss='mse',\n",
    "        metrics=RootMeanSquaredError(),\n",
    "    )\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e37f5-a930-4597-b9a0-c8fae327272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def li_to_dt(li):\n",
    "    dt = {}\n",
    "    for i, k in enumerate(param_dict.keys()):\n",
    "        if k != 'dense_dropout' and k != 'dense_leaky_relu':\n",
    "            dt[k] = int(li[i])\n",
    "        else:\n",
    "            dt[k] = li[i]\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbab1a-75cc-4612-ac73-de669000d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    LSTM,\n",
    "    SimpleRNN,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def model_builder(p_dt):\n",
    "    input_tensor = Input(shape=(p_dt[\"window_size\"], len(inputs)), name=\"input\")\n",
    "    x = input_tensor\n",
    "    if p_dt[\"conv_strides\"] == 0:\n",
    "        p_dt[\"conv_strides\"] = None\n",
    "    x = Conv1D(p_dt[\"conv_filters\"], \n",
    "               kernel_size=p_dt[\"conv_kernel_size\"], \n",
    "               kernel_initializer='he_uniform', \n",
    "               activation='relu', \n",
    "               strides=p_dt[\"conv_strides\"],\n",
    "               padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=p_dt[\"pool_size\"], \n",
    "                     strides=p_dt[\"pool_strides\"], \n",
    "                     padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(p_dt[\"dense_units\"], \n",
    "              kernel_initializer='he_uniform', \n",
    "              activation=LeakyReLU(p_dt[\"dense_leaky_relu\"]))(x)\n",
    "    x = Dropout(p_dt[\"dense_dropout\"])(x)\n",
    "    output = Dense(len(outputs), kernel_initializer='he_uniform', activation=\"relu\", name=\"output\")(x)\n",
    "\n",
    "    _model = Model(\n",
    "        inputs=input_tensor,\n",
    "        outputs=output,\n",
    "        name='test',\n",
    "    )\n",
    "\n",
    "    _model.compile(\n",
    "        optimizer=Adam(learning_rate=lr_val[p_dt[\"lr\"]]),\n",
    "        loss='mse',\n",
    "        metrics=RootMeanSquaredError(),\n",
    "    )\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad25bd-5f68-4b8e-8fb7-06abdfc76e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=5, mode=\"min\", min_lr=1e-6, verbose=False\n",
    ")\n",
    "ely_cb = EarlyStopping(monitor=\"val_loss\", patience=15, mode=\"min\", verbose=False, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadd1e4-f235-4bd8-8f66-2966b5484641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(real, pred):\n",
    "    metric_funcs = [metrics.calc_r2,\n",
    "                    metrics.calc_corrcoef, \n",
    "                    metrics.calc_nmse, \n",
    "                    metrics.calc_fb,\n",
    "                    metrics.calc_b,\n",
    "                    metrics.calc_a_co, \n",
    "                    metrics.calc_mse,]\n",
    "    res = np.zeros(len(metric_funcs))\n",
    "\n",
    "    for i, metric in enumerate(metric_funcs):\n",
    "        res[i] = metric(real, pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441365e-c6e9-4433-a743-71613deae386",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.read_csv(f'{proj_dir}/metric.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0591400-7731-4d74-926a-88d449f529ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "def to_dataset(_dfs, in_time_step):\n",
    "    return dh.dfs_to_dataset(_dfs, meta_df, inputs, outputs, in_time_step=in_time_step, out_time_step=1, offset=1, excludes=outputs)\n",
    "\n",
    "def train_model(_model):\n",
    "    _ = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=p[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        epochs=100,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[rlr_cb, ely_cb],\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f'[INFO] Finished training')\n",
    "    K.clear_session()\n",
    "\n",
    "idc = conv_idc\n",
    "metrics_indices = ['r2', 'corr', 'nmse', 'fb', 'b', 'a/c', 'mse']\n",
    "\n",
    "metric_df = pd.DataFrame(np.zeros((len(conv_idc), len(metrics_indices))), columns=metrics_indices)\n",
    "if os.path.exists(f'{proj_dir}/metric.csv'):\n",
    "    print(f'Found metric_df. Read from source.')\n",
    "    metric_df = pd.read_csv(f'{proj_dir}/metric.csv', index_col='index')\n",
    "\n",
    "\n",
    "for i, conv_idx in enumerate(idc):\n",
    "    root_dir = proj_dir+f'/trial{i:03d}'\n",
    "    if os.path.exists(root_dir):\n",
    "        continue\n",
    "    os.makedirs(root_dir)\n",
    "    print(f'[INFO] Trial{i:03d} training start')\n",
    "\n",
    "    p = li_to_dt(conv_idx)\n",
    "    with open(f\"{root_dir}/params.json\", \"w\") as outfile:\n",
    "        json.dump(p, outfile)\n",
    "        outfile.close()\n",
    "\n",
    "    win_size = p[\"window_size\"]\n",
    "    X_train, y_train = to_dataset(train_dfs, win_size)\n",
    "    X_val, y_val = to_dataset(val_dfs, win_size)\n",
    "    X_test, y_test = to_dataset(test_dfs, win_size)\n",
    "\n",
    "    y_train = y_train.reshape(-1, len(outputs))\n",
    "    y_val = y_val.reshape(-1, len(outputs))\n",
    "    y_test = y_test.reshape(-1, len(outputs))\n",
    "    model = model_builder(p)\n",
    "    train_model(model)\n",
    "\n",
    "    y_hat = model.predict(X_test, verbose=False)\n",
    "    print(f'[INFO] Trial{i:03d} finished predict')\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    del model\n",
    "    K.clear_session()\n",
    "\n",
    "    print(f'[INFO] Trial{i:03d} successfully ended.. Clear session')\n",
    "    metric = calc_metric(y_test, y_hat)\n",
    "    metric_df.iloc[i] = metric\n",
    "    metric_df.to_csv(f'{proj_dir}/metric.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef435b66-6e92-4aea-b760-f4c91481c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "def train_model(param, _model):\n",
    "    X_train = train_ds[str(param[\"window_size\"])][0]\n",
    "    y_train = train_ds[str(param[\"window_size\"])][1].reshape(-1, 3)\n",
    "    X_val = val_ds[str(param[\"window_size\"])][0]\n",
    "    y_val = val_ds[str(param[\"window_size\"])][1].reshape(-1, 3)\n",
    "    \n",
    "    history = _model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=param_values[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        epochs=100,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[rlr_cb, ely_cb],\n",
    "        verbose=False,\n",
    "    )\n",
    "    pd.DataFrame(history.history).to_csv(root_dir+'/history.csv', index=False)\n",
    "    print(f'[INFO] Finished training')\n",
    "    K.clear_session()\n",
    "\n",
    "idc = conv_idc\n",
    "param = conv_params\n",
    "metrics_indices = ['r2', 'corr', 'nmse', 'fb', 'b', 'a/c', 'mse']\n",
    "\n",
    "metric_df = pd.DataFrame(np.zeros((len(conv_idc), len(metrics_indices))), columns=metrics_indices)\n",
    "if os.path.exists(f'{proj_dir}/metric.csv'):\n",
    "    print(f'Found metric_df. Read from source.')\n",
    "    metric_df = pd.read_csv(f'{proj_dir}/metric.csv', index_col='index')\n",
    "\n",
    "\n",
    "for i, conv_idx in enumerate(idc):\n",
    "    root_dir = proj_dir+f'/trial{i:03d}'\n",
    "    if os.path.exists(root_dir):\n",
    "        continue\n",
    "    os.makedirs(root_dir)\n",
    "    print(f'[INFO] Trial{i:03d} training start')\n",
    "\n",
    "    with open(f\"{root_dir}/params.json\", \"w\") as outfile:\n",
    "        json.dump(param_values, outfile)\n",
    "        outfile.close()\n",
    "\n",
    "    X_test = test_ds[str(param_values[\"window_size\"])][0]\n",
    "    y_test = test_ds[str(param_values[\"window_size\"])][1].reshape(-1, 3)\n",
    "\n",
    "    model = model_builder(param_values, X_test[0].shape, y_test.shape[1])\n",
    "    train_model(param_values, model)\n",
    "\n",
    "    y_hat = model.predict(X_test, verbose=False)\n",
    "    print(f'[INFO] Trial{i:03d} finished predict')\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    del model\n",
    "    K.clear_session()\n",
    "\n",
    "    print(f'[INFO] Trial{i:03d} successfully ended.. Clear session')\n",
    "    metric = calc_metric(y_test, y_hat)\n",
    "    metric_df.iloc[i] = metric\n",
    "    metric_df.to_csv(f'{proj_dir}/metric.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efa6ff-0dec-4337-8863-df5ed96d2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.read_csv(f'{proj_dir}/metric.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc997cc-4a4a-42f4-846e-7a7e76bc9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = metric_df['mse'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded7860-de8a-4986-a0c6-b76b09d8cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = conv_idc\n",
    "outputs = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0ef3e-5f73-40bf-a3b4-a95c3beaf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame(inputs)\n",
    "input_df['mse'] = mse\n",
    "input_df.columns = ['window', \n",
    "                    'max_pool_size',\n",
    "                    'max_pool_strides',\n",
    "                    'dense_units',\n",
    "                    'dense_dropout',\n",
    "                    'leaky_relu',\n",
    "                    'batch_size',\n",
    "                    'lr',\n",
    "                    'filter_size_1',\n",
    "                    'kernel_size_1',\n",
    "                    'strides_1',\n",
    "                    'filter_size_2',\n",
    "                    'kernel_size_2',\n",
    "                    'strides_2',\n",
    "                    'mse'\n",
    "                   ]\n",
    "dt = dict_cat(conv_params)\n",
    "input_df['window'] = np.array(dt['window_size'])[input_df['window'].values]\n",
    "input_df['max_pool_size'] = np.array(dt['pool_size'])[input_df['max_pool_size'].values]\n",
    "input_df['max_pool_strides'] = np.array(dt['pool_strides'])[input_df['max_pool_strides'].values]\n",
    "input_df['dense_units'] = np.array(dt['dense']['units'])[input_df['dense_units'].values]\n",
    "input_df['dense_dropout'] = np.array(dt['dense']['dropout'])[input_df['dense_dropout'].values]\n",
    "input_df['leaky_relu'] = np.array(dt['dense']['leaky_relu'])[input_df['leaky_relu'].values]\n",
    "input_df['lr'] = np.array(dt['lr'])[input_df['lr'].values]\n",
    "input_df['batch_size'] = np.array(dt['batch_size'])[input_df['batch_size'].values]\n",
    "input_df['filter_size_1'] = np.array(dt['conv_0']['filters'])[input_df['filter_size_1'].values]\n",
    "input_df['kernel_size_1'] = np.array(dt['conv_0']['kernel_size'])[input_df['kernel_size_1'].values]\n",
    "input_df['strides_1'] = np.array(dt['conv_0']['strides'])[input_df['strides_1'].values]\n",
    "input_df['filter_size_2'] = np.array(dt['conv_1']['filters'])[input_df['filter_size_2'].values]\n",
    "input_df['kernel_size_2'] = np.array(dt['conv_1']['kernel_size'])[input_df['kernel_size_2'].values]\n",
    "input_df['strides_2'] = np.array(dt['conv_1']['strides'])[input_df['strides_2'].values]\n",
    "input_df = input_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24511cd-794a-47c0-bc3f-c1afb0a6785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_df.iloc[:, :-1].values, mse, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c368b-98ce-4176-89ab-9182f627c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, criterion='squared_error', max_depth=10)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89864207-1bbd-48dc-8a6c-cd9ba554fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_idx = model.feature_importances_.argsort()\n",
    "plt.barh(input_df.columns[:-1][sorted_idx], model.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676aca57-a6ed-4b09-bb28-872f00c51ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_0 = input_df[input_df['lr'] == 0.001]\n",
    "lr_1 = input_df[input_df['lr'] == 0.0001]\n",
    "lr_2 = input_df[input_df['lr'] == 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600e1fe-a339-4794-a7d6-daf48cd3a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(30, 8))\n",
    "# lr_0[lr_0['mse'] < 1000].hist('mse', bins=100, ax=axes[0])\n",
    "# lr_1[lr_1['mse'] < 1000].hist('mse', bins=100, ax=axes[1])\n",
    "# lr_2[lr_2['mse'] < 1000].hist('mse', bins=100, ax=axes[2])\n",
    "\n",
    "lr_0.hist('mse', bins=100, ax=axes[0])\n",
    "lr_1.hist('mse', bins=100, ax=axes[1])\n",
    "lr_2.hist('mse', bins=100, ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa35e3-b292-4c23-b2e5-a5c7b0f0ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_0 = input_df[input_df['filter_size_2'] == 0]\n",
    "filter_1 = input_df[input_df['filter_size_2'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b2634-1e72-410d-88c4-01dd07c953d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 6))\n",
    "# lr_0[lr_0['mse'] < 1000].hist('mse', bins=100, ax=axes[0])\n",
    "# lr_1[lr_1['mse'] < 1000].hist('mse', bins=100, ax=axes[1])\n",
    "# lr_2[lr_2['mse'] < 1000].hist('mse', bins=100, ax=axes[2])\n",
    "\n",
    "filter_0.hist('mse', bins=100, ax=axes[0])\n",
    "filter_1.hist('mse', bins=100, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51516e60-9aaf-4deb-b092-80ea2e669f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_0.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf0262-daf0-4add-8cca-e8b370b84220",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1de0fa-dfaf-4374-846b-4ee88779e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(model, X_test, y_test)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(input_df.columns[:-1][sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e7ee2-fa91-4cb1-bd60-688fa8695b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_df = input_df.copy()\n",
    "# for i in range(input_df.shape[1]):\n",
    "#     for j in range(input_df.shape[1]):\n",
    "#         cp_df[f'{i}*{j}'] = cp_df[i].values * cp_df[j].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95293fc-51f7-4645-b464-0afca66d1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_input_df.iloc[:, :-1].values, mse, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc799248-3714-49fa-8f1e-65a0ed66d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# new_inputs = cp_df.values\n",
    "\n",
    "poly_model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=True)),\n",
    "    (\"lin_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "poly_model.fit(X_train, y_train)\n",
    "\n",
    "print(poly_model.score(X_train, y_train))\n",
    "print(poly_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d72e1-977c-4d1f-8744-b6773afc58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8707e-3522-4e15-95e1-d3d6eb6bc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.sample(int(len(input_df) * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553e002-848b-4b79-a0c2-73550d2d7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# basedir = f'{proj_dir}'\n",
    "\n",
    "# dirs = sorted(os.listdir(basedir))[4:]\n",
    "# for d in dirs:\n",
    "#     name = d[:5]\n",
    "#     num = int(d[5:])\n",
    "#     print(d, f'{num:05d}')\n",
    "#     os.rename(os.path.join(basedir, d), os.path.join(basedir, f'trial{num:05d}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f7f9c-e4f5-4e6b-87eb-592bc27ca48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7f961-e381-471b-9b4a-b421fab3f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e163c1c-0d19-479b-9ba8-88ced27dea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(model.estimators_[0], out_file='tree.dot', \n",
    "                feature_names=input_df.columns[:-1],\n",
    "                class_names='mse',\n",
    "                rounded=True, proportion=False, \n",
    "                precision=2, filled=True)\n",
    "\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f28f4e-a138-4521-b7fb-dd8a825097aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
